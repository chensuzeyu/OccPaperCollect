{
    "title":  "ImagineNav: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination",
    "short_title":  "ImagineNav",
    "authors":  [
                    "Xinxin Zhao",
                    "Wenzhe Cai",
                    "Likun Tang",
                    "Teng Wang"
                ],
    "first_author":  "Zhao",
    "venue":  "ICLR 2025",
    "presentation_type":  "poster",
    "openreview_url":  "https://openreview.net/forum?id=vQFw9ryKyK",
    "openreview_id":  "vQFw9ryKyK",
    "pdf_url":  "/pdf/e349d69236fa6d97f504e96881ee34405d7de516.pdf",
    "abstract":  "Visual navigation is an essential skill for home-assistance robots, providing the object-searching ability to accomplish long-horizon daily tasks. Many recent approaches use Large Language Models (LLMs) for commonsense inference to improve exploration efficiency. However, the planning process of LLMs is limited within texts and it is difficult to represent the spatial occupancy and geometry layout only by texts. Both are important for making rational navigation decisions. In this work, we seek to unleash the spatial perception and planning ability of Vision-Language Models (VLMs), and explore whether the VLM, with only on-board camera captured RGB/RGB-D stream inputs, can efficiently finish the visual navigation tasks in a mapless manner. We achieve this by developing the imagination-powered navigation framework ImagineNav, which imagines the future observation images at valuable robot views and translates the complex navigation planning process into a rather simple best-view image selection problem for VLM. To generate appropriate candidate robot views for imagination, we introduce the Where2Imagine module, which is distilled to align with human navigation habits. Finally, to reach the VLM preferred views, an off-the-shelf point-goal navigation policy is utilized. Empirical experiments on the challenging open-vocabulary object navigation benchmarks demonstrates the superiority of our proposed system.",
    "keywords":  [
                     "Robotics",
                     "Visual Navigation",
                     "Vision-Language Model",
                     "Scene Imagination"
                 ],
    "local_pdf":  "ImagineNav_ICLR2025.pdf"
}
{
  "title": "Adapting to Stochastic and Adversarial Losses in Episodic MDPs with Aggregate Bandit Feedback",
  "short_title": "Adapting to Stochastic and Adv",
  "authors": [
    "Shinji Ito",
    "Kevin Jamieson",
    "Haipeng Luo",
    "Arnab Maiti",
    "Taira Tsuchiya"
  ],
  "first_author": "Ito",
  "venue": "NeurIPS 2025",
  "presentation_type": "poster",
  "openreview_url": "https://openreview.net/forum?id=C3sKfe8e1n",
  "openreview_id": "C3sKfe8e1n",
  "pdf_url": "/pdf/2b77b424730975f763e3d0a24fbb3d9dcd3fab4b.pdf",
  "abstract": "We study online learning in finite-horizon episodic Markov decision processes (MDPs) under the challenging \\textit{aggregate bandit feedback} model,\nwhere the learner observes only the cumulative loss incurred in each episode,\nrather than individual losses at each state-action pair.\nWhile prior work in this setting has focused exclusively on worst-case analysis,\nwe initiate the study of \\textit{best-of-both-worlds} (BOBW) algorithms that achieve low regret in both stochastic and adversarial environments.\nWe propose the first BOBW algorithms for episodic tabular MDPs with aggregate bandit feedback.\nIn the case of known transitions,\nour algorithms achieve $O(\\log T)$ regret in stochastic settings and ${O}(\\sqrt{T})$ regret in adversarial ones.\nImportantly, we also establish matching lower bounds, showing the optimality of our algorithms in this setting.\nWe further extend our approach to unknown-transition settings by incorporating confidence-based techniques.\nOur results rely on a combination of FTRL over occupancy measures,\nself-bounding techniques,\nand new loss estimators inspired by recent advances in online shortest path problems.\nAlong the way,\nwe also provide the first individual-gap-dependent lower bounds and demonstrate near-optimal BOBW algorithms for shortest path problems with bandit feedback.",
  "keywords": [
    "online MDPs",
    "aggregate feedback",
    "best of both worlds"
  ],
  "local_pdf": ""
}
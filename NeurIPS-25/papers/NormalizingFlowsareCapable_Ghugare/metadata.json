{
  "title": "Normalizing Flows are Capable Models for Continuous Control",
  "short_title": "Normalizing Flows are Capable",
  "authors": [
    "Raj Ghugare",
    "Benjamin Eysenbach"
  ],
  "first_author": "Ghugare",
  "venue": "NeurIPS 2025",
  "presentation_type": "poster",
  "openreview_url": "https://openreview.net/forum?id=EJ34X5VWwu",
  "openreview_id": "EJ34X5VWwu",
  "pdf_url": "/pdf/fb3aa8a0b0393fb8bbdda14e19034e9883eca2bc.pdf",
  "abstract": "Modern reinforcement learning (RL) algorithms have found success by using powerful probabilistic models, such as transformers, energy-based models, and diffusion/flow-based models. To this end, RL researchers often choose to pay the price of accommodating these models into their algorithms -- diffusion models are expressive, but are computationally intensive due to their reliance on solving differential equations, while autoregressive transformer models are scalable but typically require learning discrete representations. Normalizing flows (NFs), by contrast, seem to provide an appealing alternative, as they enable likelihoods and sampling without solving differential equations or autoregressive architectures. However, their potential in RL has received limited attention, partly due to the prevailing belief that normalizing flows lack sufficient expressivity. We show that this is not the case. Building on recent work in NFs, we propose a single NF architecture which integrates seamlessly into RL algorithms, serving as a policy, Q-function, and occupancy measure. Our approach leads to much simpler algorithms, and achieves higher performance in imitation learning, offline, goal conditioned RL and unsupervised RL.",
  "keywords": [
    "Reinforcement Learning",
    "Normalizing Flows",
    "Control as Inference",
    "Offline RL",
    "Imitation Learning",
    "Unsupervised Goal Conditioned RL"
  ],
  "local_pdf": ""
}